{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Physics 256\n",
    "## Curve Fitting via Least Squares Minimization\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/6/68/Overfitted_Data.png\" width=600px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import style\n",
    "style._set_css_style('../include/bootstrap.css')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last Time\n",
    "### [Notebook Link: 19_LinearRegression](./19_LinearRegression.ipynb)\n",
    "\n",
    "- Definition of fitting residuals and $\\chi^2$\n",
    "- Derived optimal values for linear regression\n",
    "\n",
    "\n",
    "## Today\n",
    "\n",
    "- Making things look linear\n",
    "- Non-linear regression\n",
    "- Goodness of fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "plt.style.use('../include/notebook.mplstyle');\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping non-linear problems onto linear ones \n",
    "\n",
    "Last time we derived the formulae for $a_0$ and $a_1$ corresponding to a linear fitting function:\n",
    "\n",
    "\\begin{equation}\n",
    "Y(x;\\vec{a}) = a_0 + a_1 x .\n",
    "\\end{equation}\n",
    "\n",
    "While today we will learn how to perform fits to general functions, there are many seemingly non-linear cases that can be mapped to linear ones with an appropriate choice of variables.\n",
    "\n",
    "### 1. Exponentials: $Y(x;\\vec{a}) = a_0 \\mathrm{e}^{a_1 x}$\n",
    "\n",
    "Define: \n",
    "\\begin{equation}\n",
    "\\tilde{Y}(x;\\vec{\\tilde{a}}) = \\ln Y\n",
    "\\end{equation}\n",
    "we can write:\n",
    "\\begin{equation}\n",
    "\\tilde{Y}(x;\\vec{\\tilde{a}}) = \\tilde{a}_0 + \\tilde{a}_1 x\n",
    "\\end{equation}\n",
    "where\n",
    "\\begin{align}\n",
    "\\tilde{a}_0 &= \\ln a_0 \\newline\n",
    "\\tilde{a}_1 &= a_1.\n",
    "\\end{align}\n",
    "\n",
    "### 2. Powerlaws: $Y(x;\\vec{a}) = a_0 x^{a_1}$\n",
    "\n",
    "Define:\n",
    "\\begin{equation}\n",
    "\\tilde{Y}(x;\\vec{\\tilde{a}}) = \\ln Y\n",
    "\\end{equation}\n",
    "we can write:\n",
    "\\begin{equation}\n",
    "\\tilde{Y}(\\tilde{x};\\vec{\\tilde{a}}) = \\tilde{a}_0 + \\tilde{a}_1 \\tilde{x}\n",
    "\\end{equation}\n",
    "where\n",
    "\\begin{align}\n",
    "\\tilde{x} &= \\ln x \\newline\n",
    "\\tilde{a}_0 &= \\ln a_0 \\newline\n",
    "\\tilde{a}_1 &= a_1.\n",
    "\\end{align}\n",
    "\n",
    "<br />\n",
    "<div class=\"span alert-danger\">\n",
    "Note that in both cases, we may have to shift our data to make it go through the origin.\n",
    "</div>\n",
    "\n",
    "## Example:\n",
    "\n",
    "<!--\n",
    "np.array([[cx,2.4*np.exp(-1.7*cx) + np.random.normal(0,0.005/cx),0.2*cx**1.2 + np.random.normal(0,0.05)] for cx in np.linspace(0.1,5,20)])\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# We defined these functions last time\n",
    "def Σ(σ,q):\n",
    "    '''Compute the Σ function needed for linear fits.'''\n",
    "    return np.sum(q/σ**2)\n",
    "\n",
    "def get_a(x,y,σ):\n",
    "    '''Get the χ^2 best fit value of a0 and a1.'''\n",
    "\n",
    "    # Get the individual Σ values\n",
    "    Σy,Σx,Σx2,Σ1,Σxy = Σ(σ,y),Σ(σ,x),Σ(σ,x**2),Σ(σ,np.ones(x.size)),Σ(σ,x*y)\n",
    "\n",
    "    # the denominator\n",
    "    D = Σ1*Σx2 - Σx**2\n",
    "\n",
    "    # compute the best fit coefficients\n",
    "    a = np.array([Σy*Σx2 - Σx*Σxy,Σ1*Σxy - Σx*Σy])/D\n",
    "\n",
    "    # Compute the error in a\n",
    "    aErr = np.array([np.sqrt(Σx2/D),np.sqrt(Σ1/D)])\n",
    "\n",
    "    return a,aErr\n",
    "\n",
    "def linear(x,a):\n",
    "    '''Return a polynomial of order'''\n",
    "    return a[0] + a[1]*x\n",
    "\n",
    "data = np.array([[  1.00000000e-01,   2.04138220e+00,   9.73629324e-03],\n",
    "       [  3.57894737e-01,   1.30119078e+00,   1.12856801e-01],\n",
    "       [  6.15789474e-01,   8.42154689e-01,   9.83121201e-02],\n",
    "       [  8.73684211e-01,   5.45192601e-01,   1.29185248e-01],\n",
    "       [  1.13157895e+00,   3.59854509e-01,   2.15797712e-01],\n",
    "       [  1.38947368e+00,   2.23469107e-01,   2.81326486e-01],\n",
    "       [  1.64736842e+00,   1.47065865e-01,   3.29001539e-01],\n",
    "       [  1.90526316e+00,   9.42222066e-02,   4.24678699e-01],\n",
    "       [  2.16315789e+00,   6.29051329e-02,   4.21412036e-01],\n",
    "       [  2.42105263e+00,   3.49731098e-02,   6.30215689e-01],\n",
    "       [  2.67894737e+00,   2.37533207e-02,   5.75886026e-01],\n",
    "       [  2.93684211e+00,   1.54965698e-02,   7.41655404e-01],\n",
    "       [  3.19473684e+00,   9.73289991e-03,   8.07876091e-01],\n",
    "       [  3.45263158e+00,   8.36780173e-03,   8.86926901e-01],\n",
    "       [  3.71052632e+00,   3.90242054e-03,   9.43666008e-01],\n",
    "       [  3.96842105e+00,   1.61554262e-03,   1.13278970e+00],\n",
    "       [  4.22631579e+00,   2.59857424e-03,   1.14161518e+00],\n",
    "       [  4.48421053e+00,   1.73614348e-03,   1.23615458e+00],\n",
    "       [  4.74210526e+00,   1.63584150e-03,   1.25628767e+00],\n",
    "       [  5.00000000e+00,   1.59297834e-03,   1.27049395e+00]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# get the data\n",
    "x,y1,y2,σ = data[:,0],data[:,1],data[:,2],np.ones_like(data[:,0])\n",
    "\n",
    "# plot the data\n",
    "plt.plot(x,y1,'o',mfc=colors[0], mec='None',label='set 1')\n",
    "plt.plot(x,y2,'s',mfc=colors[1], mec='None', label='set 2')\n",
    "\n",
    "# peform the fits\n",
    "a1,a1_err = get_a(x[:-10],np.log(y1[:-10]),σ[:-10])\n",
    "a2,a2_err = get_a(np.log(x),np.log(y2),σ)\n",
    "\n",
    "# plot the fit results\n",
    "fx = np.linspace(0,5,100)\n",
    "plt.plot(fx,np.exp(a1[0])*np.exp(a1[1]*fx), color=colors[0], linewidth=1.5, zorder=0, label='set 1 fit')\n",
    "plt.plot(fx,np.exp(a2[0])*fx**a2[1], color=colors[1],linewidth=1.5, zorder=0, label='set 2 fit')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"span alert-danger\">\n",
    "You need to be careful with the correct propagation of errors in your fitting parameteters when doing this!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Least Squares Fitting\n",
    "\n",
    "We can easily generalize our fitting procedure to functions of the form:\n",
    "\n",
    "\\begin{equation}\n",
    "Y(x;\\vec{a}) = \\sum_{\\alpha=0}^{M-1} a_\\alpha Y_\\alpha(x)\n",
    "\\end{equation}\n",
    "where $Y_\\alpha(x) : \\mathbb{R} \\to \\mathbb{R}$.  As before, we minimize $\\chi^2$ with respect to $a_\\alpha$:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\chi^2}{\\partial a_\\alpha} &= \\frac{\\partial}{\\partial a_\\alpha} \\sum_i \\frac{1}{\\sigma_i^2}\\left[\\sum_{\\beta=0}^{M-1}a_\\beta Y_\\beta(x_i)  y_i\\right]^2 = 0\\newline\n",
    "&= \\sum_i \\frac{Y_\\alpha(x_i)}{\\sigma_i^2}\\left[\\sum_{\\beta=0}^{M-1}a_\\beta Y_\\beta(x_i)  y_i\\right] = 0\\newline\n",
    "\\end{align}\n",
    "which gives:\n",
    "\\begin{equation}\n",
    "\\sum_i \\sum_\\beta \\frac{Y_\\alpha(x_i)Y_\\beta(x_i)}{\\sigma_i^2} a_\\beta = \\sum_i \\frac{Y_\\alpha(x_i)y_i}{\\sigma_i^2} .\n",
    "\\end{equation}\n",
    "\n",
    "At this point it is useful to define the **design matrix**   $\\mathsf{A}$ which is independenct of $y_i$ and only depends on where the data is measured:\n",
    "\\begin{equation}\n",
    "\\mathsf{A}_{i\\alpha} \\equiv \\frac{Y_\\alpha(x_i)}{\\sigma_i}.\n",
    "\\end{equation}\n",
    "\n",
    "We can re-write our optimization equation as:\n",
    "\\begin{equation}\n",
    "\\sum_i \\sum_\\beta \\mathsf{A}_{i\\alpha}\\mathsf{A}_{i\\beta} a_\\beta = \\sum_i \\mathsf{A}_{i\\alpha}\\frac{y_i}{\\sigma_i}\n",
    "\\end{equation}\n",
    "which is much simpler using matrix multiplication:\n",
    "\\begin{equation}\n",
    "\\left(\\mathsf{A}^\\intercal \\mathsf{A}\\right) \\vec{a} = \\mathsf{A}^\\intercal \\vec{b}\n",
    "\\end{equation}\n",
    "where $b_i \\equiv y_i/\\sigma_i$. This equation can be easily solved:\n",
    "\n",
    "\\begin{equation}\n",
    "\\vec{a} =\\left(\\mathsf{A}^\\intercal \\mathsf{A}\\right)^{-1} \\mathsf{A}^\\intercal \\vec{b}\n",
    "\\end{equation}\n",
    "where the uncertainty in the fitting parameters are given by the covariance matrix:\n",
    "\\begin{align}\n",
    "\\sigma_{a_{\\alpha}} &= \\sqrt{\\mathsf{C}_{\\alpha\\alpha}} \\newline\n",
    "\\mathsf{C} &= \\left(\\mathsf{A}^\\intercal \\mathsf{A}\\right)^{-1} .\n",
    "\\end{align}\n",
    "\n",
    "This is how *all* black-box least squares fitting functions work.  We will use `curve_fit` from `scipy.optimize`: \n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "\n",
    "def exp_func(x,*a):\n",
    "    '''exponential function'''\n",
    "    return a[0]*np.exp(a[1]*x)\n",
    "\n",
    "def power_func(x,*a):\n",
    "    '''power function.'''\n",
    "    return a[0]*x**a[1]\n",
    "\n",
    "# perform the fits\n",
    "a1,a1_cov = curve_fit(exp_func,x,y1,p0=(1,1))\n",
    "a2,a2_cov = curve_fit(power_func,x,y2,p0=(1,1))\n",
    "\n",
    "# plot the data\n",
    "plt.plot(x,y1,'o',mfc=colors[0], mec='None',label='set 1')\n",
    "plt.plot(x,y2,'s',mfc=colors[1], mec='None', label='set 2')\n",
    "\n",
    "# plot the fit results\n",
    "fx = np.linspace(0,5,100)\n",
    "plt.plot(fx,exp_func(fx,*a1), color=colors[0], linewidth=1.5, zorder=0, label = 'set 1 fit')\n",
    "plt.plot(fx,power_func(fx,*a2), color=colors[1],linewidth=1.5, zorder=0, label='set 2 fit')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More interesting data: thermal conductivity of copper\n",
    "\n",
    "We know that heat conduction in metals is due to (1) electronic and (2) phononic contributions. These can be distinguished by their temperature dependence and the thermal conductivity is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "\\kappa(T) = \\frac{1}{a_0/T + a_1 T^2} .\n",
    "\\end{equation}\n",
    "\n",
    "<div class=\"span alert alert-success\">\n",
    "<h2> Programming challenge </h2>\n",
    "    Load the experimental data for the thermal conductivity of copper  <kbd>../data/kappa.dat</kbd>. Perform a non-linear fit to the data and extract the two parameters with error bars.  How good is your fit?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def κ_theory(T,*a):\n",
    "    '''Model for thermal conductivity'''\n",
    "    pass\n",
    "    return 0.0\n",
    "\n",
    "# Load the data and perform the fit\n",
    "pass\n",
    "\n",
    "# plot the original data\n",
    "pass\n",
    "\n",
    "# plot the line of best fit\n",
    "fit_T = np.linspace(0.1,60,1000)\n",
    "pass\n",
    "\n",
    "# add axis labels and legend\n",
    "pass\n",
    "plt.xlabel('Temperature [K]')\n",
    "plt.ylabel('Thermal Conductivity [W/cm K]')\n",
    "plt.legend()\n",
    "\n",
    "# fit values\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How good is this fit?  \n",
    "We can calculate the value of $\\chi^2$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\chi^2 = \\sum_i \\frac{1}{\\sigma_i^2} \\left[Y(x_i;\\vec{a})-y_i \\right]^2\n",
    "\\end{equation}\n",
    "\n",
    "and we expect that a *good fit* should reflect the fact that the distance between the curve and data points should be on the order of the errorbars:\n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma_i \\approx |Y(x_i;\\vec{a})-y_i|\n",
    "\\end{equation}\n",
    "\n",
    "thus we find:\n",
    "\n",
    "\\begin{equation}\n",
    "\\chi^2 \\approx N.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def χ2(x,y,Y,σ=None):\n",
    "    '''Return the value of χ².'''\n",
    "    if σ.any():\n",
    "        return np.sum(((Y-y)/σ)**2)\n",
    "    else: \n",
    "        return np.sum((Y-y)**2)\n",
    "   \n",
    "print(f'κ-fit: χ² = {χ2(T,κ,κ_theory(T,*a),σ):5.3E}')\n",
    "print(T.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *bare* value of $\\chi^2$ can be a bit misleading, as we know that the more parameters we use in the fit, the better the fit will be.  The fit will be **perfect** when $N=M$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def poly_fit(x,*par):\n",
    "    '''A par.size-1 order polynomial'''\n",
    "    poly = np.poly1d(par)\n",
    "    return poly(x)\n",
    "\n",
    "# perform a pathalogical fit to the data\n",
    "M = 7\n",
    "ap,ap_cov = curve_fit(poly_fit,T,κ,p0=np.ones(M))\n",
    "\n",
    "# plot the original data\n",
    "plt.errorbar(T,κ,yerr=σ, marker='o', mfc=colors[2], mec='w', markersize=8, \n",
    "             linestyle='none', capsize=0, elinewidth=1.5, ecolor=colors[2], label='Cu Data')\n",
    "\n",
    "# plot the fits\n",
    "fit_T = np.linspace(0.1,60,1000)\n",
    "plt.plot(fit_T, κ_theory(fit_T,*a), '-', color=colors[2], linewidth=2, \n",
    "         zorder=0, label=r'$(a_0/T + a_1 T\\ {}^2)^{-1}$')\n",
    "plt.plot(fit_T, poly_fit(fit_T,*ap), '--', color=colors[2], linewidth=2, \n",
    "         zorder=0, label='Order-%d polynomial'%(M-1))\n",
    "\n",
    "# add axis labels and legend\n",
    "plt.xlabel('Temperature [K]')\n",
    "plt.ylabel('Thermal Conductivity [W/cm K]')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus it is meaninful to consider the value of $\\chi^2$ per *degree of freedom* (DOF), i.e.:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\chi^2}{N-M}\n",
    "\\end{equation}\n",
    "\n",
    "We say that a good fit has $\\chi^2/{\\rm DOF} \\sim 1$. If:\n",
    "1. $\\chi^2/DOF \\gg 1$: not using an appropriate model to fit, or the error bars are too large. \n",
    "2. $\\chi^2/DOF \\ll 1$: overfitting, error bars may be too small.\n",
    "\n",
    "A great resource for learning more about curve fitting is: [Peter Young, *Everything you wanted to know about Data Analysis and Fitting but were afraid to ask*](https://arxiv.org/abs/1210.3781).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(f'κ-fit      : χ² = {χ2(T,κ,κ_theory(T,*a),σ)/(T.size-2):G}') \n",
    "print(f'{M-1}-poly-fit : χ² = {χ2(T,κ,poly_fit(T,*ap),σ)/(T.size-M):G}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Experts\n",
    "\n",
    "The **goodness of fit** parameter, $Q$, measures the probability that this value of $\\chi^2$, or greater, could occur by chance, assuming that the data points are normally distributed.\n",
    "\n",
    "\\begin{equation}\n",
    "Q = \\frac{1}{\\Gamma(N_{DOF}/2)} \\int_{\\chi^2/2}^{\\infty} dy y^{N_{DOF}/2-1} \\mathrm{e}^{-y} .\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from scipy.special import gammaincc\n",
    "print(f'κ-fit      : Q = {gammaincc(0.5*(T.size-2), 0.5*χ2(T,κ,κ_theory(T,*a),σ)):5.3f}')\n",
    "print(f'{M-1}-poly-fit : Q = {gammaincc(0.5*(T.size-M), 0.5*χ2(T,κ,poly_fit(T,*ap),σ)):5.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
