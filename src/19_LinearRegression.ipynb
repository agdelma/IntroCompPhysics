{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Physics 256\n",
    "## Linear Regression and Least Squares Fitting\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Linear_regression.svg/438px-Linear_regression.svg.png\" width=600px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last Time\n",
    "\n",
    "### [Notebook Link: 18_Logistic Map](./18_Logistic_Map.ipynb)\n",
    "\n",
    "- The logistic map\n",
    "- a simple route to chaotic behavior\n",
    "- period doubling, fixed points and attractors\n",
    "\n",
    "\n",
    "## Today\n",
    "\n",
    "- An introduction to the theory of curve fitting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "plt.style.use('../include/notebook.mplstyle');\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least Squares Analysis \n",
    "Science is exceedingly becoming a *data rich* pursuit and modern experiments or simulations can easily produce terabytes of data.  Curve fitting or regression analysis is the simplest type of *data analysis*.\n",
    "\n",
    "We begin by considering $N$ two dimensional data points:\n",
    "\n",
    "$$\\{(x_0,y_0),\\ldots,(x_{N-1},y_{N-1})\\} $$\n",
    "\n",
    "and we want to fit this data to a continuous function:\n",
    "\n",
    "$$Y(x;\\vec{a})$$\n",
    "\n",
    "where $\\vec{a}$ is a vector of $M$ adjustable parameters that we want to determine such that our data **fits** the function.   \n",
    "\n",
    "Intuitively, this means that our data points should fall *near* the curve $Y$, we can quantify by defining the residual $\\Delta_i$ for each point:\n",
    "\n",
    "\\begin{equation}\n",
    "\\Delta_i \\equiv Y(x_i;\\vec{a}) - y_i .\n",
    "\\end{equation}\n",
    "\n",
    "Our approach will be to find $\\vec{a}$ such that it minimizes the **squares** of these errors:  \n",
    "\\begin{equation}\n",
    "\\sum_{i=0}^{N-1} \\Delta_i^2 = \\sum_{i=0}^{N-1} \\left[Y(x_i;\\vec{a}) - y_i\\right]^2 .\n",
    "\\end{equation}\n",
    "This is known as **least-squares fitting** and it was first used by Gauss to determine the orbits of the comets from observational data.\n",
    "\n",
    "If there are uncertainties in the data such that $y_i \\to y_i \\pm \\sigma_i$ then we need to modify this equation to place less emphasis on those data points with large error.  This defines the $\\chi^2$ residual:\n",
    "\n",
    "\\begin{equation}\n",
    "\\chi^2 = \\sum_{i=0}^{N-1} \\left(\\frac{\\Delta_i}{\\sigma_i}\\right)^2 = \\sum_{i=0}^{N-1} \\frac{1}{\\sigma_i^2} \\left[Y(x_i;\\vec{a}) - y_i\\right]^2 .\n",
    "\\end{equation}\n",
    "\n",
    "If we assume that the errors $\\sigma_i$ are normally distributed, we can make concrete statistical statements about the goodness of fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anscombe's quartet\n",
    "\n",
    "Before we learn how to do this, it is important to note that fitting a curve to a given dataset does not in of itself produce anything meaningful.  One must carefully ask the question:\n",
    "\n",
    "*What is the probability that the data, given their uncertainties, are described by the curve?*\n",
    "\n",
    "This is the statistical science of hypothesis testing.  We will investigate a linear fit to a [special data set](https://en.wikipedia.org/wiki/Anscombe%27s_quartet) that highlights the need to actually graph our data and consider the fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def linear(x,a):\n",
    "    '''A linear fitting function.'''\n",
    "    return a[0] + a[1]*x\n",
    "\n",
    "def χ2(Y,y):\n",
    "    '''Unscaled χ^2'''\n",
    "    return np.sum((Y-y)**2)\n",
    "\n",
    "anscombe = np.array([[10.0,8.04,10.0,9.14,10.0,7.46,8.0,6.58],[8.0,6.95,8.0,8.14,8.0,6.77,8.0,5.76],\n",
    "[13.0,7.58,13.0,8.74,13.0,12.74,8.0,7.71],[9.0,8.81,9.0,8.77,9.0,7.11,8.0,8.84],\n",
    "[11.0,8.33,11.0,9.26,11.0,7.81,8.0,8.47],[14.0,9.96,14.0,8.10,14.0,8.84,8.0,7.04],\n",
    "[6.0,7.24,6.0,6.13,6.0,6.08,8.0,5.25],[4.0,4.26,4.0,3.10,4.0,5.39,19.0,12.50],\n",
    "[12.0,10.84,12.0,9.13,12.0,8.15,8.0,5.56],[7.0,4.82,7.0,7.26,7.0,6.42,8.0,7.91],\n",
    "[5.0,5.68,5.0,4.74,5.0,5.73,8.0,6.8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,2,sharex=True, sharey=True, squeeze=False, figsize=(10,7))\n",
    "plt.subplots_adjust(hspace=0.05,wspace=0.05)\n",
    "x = np.linspace(4,20,1000)\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    Y = linear(x,(3,0.5))\n",
    "    ax.plot(x,Y,'-',color=colors[0], linewidth=2, label='y = 0.5x + 3')\n",
    "    r2 = χ2(linear(anscombe[:,2*i],(3,0.5)),anscombe[:,2*i+1])\n",
    "    ax.plot(anscombe[:,2*i],anscombe[:,2*i+1],'o', markeredgecolor='None', alpha=0.7, markersize=7,\n",
    "           label=f'$\\chi^2 = {r2:4.2f}$', mfc=colors[2])\n",
    "    ax.legend(loc='lower right')\n",
    "\n",
    "[ax.set_ylabel('y') for ax in axes[:,0]]\n",
    "[ax.set_xlabel('x') for ax in axes[-1,:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "How were the coefficients in the Ancombe Quartet linear fit determined?  \n",
    "\n",
    "Let us investigate $\\chi^2$ for a linear fitting function defined by:\n",
    "\n",
    "\\begin{equation}\n",
    "Y(x;a_0,a_1) = a_0 + a_1 x\n",
    "\\end{equation}\n",
    "\n",
    "such that\n",
    "\n",
    "\\begin{equation}\n",
    "\\chi^2 = \\sum_{i=0}^{N-1} \\frac{1}{\\sigma_i^2} \\left(a_0 + a_1 x_i - y_i\\right)^2 .\n",
    "\\end{equation}\n",
    "\n",
    "We want to minimize $\\chi^2$ over the parameters $\\vec{a}$:\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\chi^2}{\\partial a_0} &= 2 \\sum_i \\frac{1}{\\sigma_i^2}(a_0 + a_1 x_i - y_i) = 0\\newline\n",
    "\\frac{\\partial \\chi^2}{\\partial a_1} &= 2 \\sum_i \\frac{1}{\\sigma_i^2}(a_0 + a_1 x_i - y_i)x_i = 0 .\n",
    "\\end{align}\n",
    "\n",
    "Defining:\n",
    "\\begin{equation}\n",
    "\\Sigma_q = \\sum_i \\frac{q_i}{\\sigma_i^2}\n",
    "\\end{equation}\n",
    "and solving these two equations yields:\n",
    "\n",
    "\\begin{align}\n",
    "a_0 &= \\frac{\\Sigma_y\\Sigma_{x^2} - \\Sigma_x \\Sigma_{xy}}{\\Sigma_1\\Sigma_{x^2} - (\\Sigma_x)^2 } \\newline\n",
    "a_1 &= \\frac{\\Sigma_1\\Sigma_{xy} - \\Sigma_y \\Sigma_{x}}{\\Sigma_1\\Sigma_{x^2} - (\\Sigma_x)^2 } .\n",
    "\\end{align}\n",
    "\n",
    "**Note:** If the errors are constant, i.e. $\\sigma_i = \\sigma\\ \\forall\\ i=0,\\ldots,N-1$ then it drops out of these expressions.  Thus we can use this formulation for fitting even if we don't know the uncertainties by setting $\\sigma_i = 1$.\n",
    "\n",
    "### Computing the uncertainty in $\\vec{a}$\n",
    "\n",
    "We can determine the uncertainty or error in our fit parameters using conventional error propagation resulting in:\n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma^2_{a_\\alpha} = \\sum_i \\left( \\frac{\\partial a_\\alpha}{\\partial y_i}\\right)^2 \\sigma_i^2 .\n",
    "\\end{equation}\n",
    "\n",
    "We find:\n",
    "\n",
    "\\begin{align}\n",
    "\\sigma^2_{a_0} &= \\frac{\\Sigma_{x^2}}{\\Sigma_1\\Sigma_{x^2} - (\\Sigma_x)^2} \\newline\n",
    "\\sigma^2_{a_1} &= \\frac{\\Sigma_{1}}{\\Sigma_1\\Sigma_{x^2} - (\\Sigma_x)^2} .\n",
    "\\end{align}\n",
    "\n",
    "### Let's write some functions to perform a linear least-squares fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def Σ(σ,q):\n",
    "    '''Compute the Σ function needed for linear fits.'''\n",
    "    return np.sum(q/σ**2)\n",
    "\n",
    "def get_a(x,y,σ):\n",
    "    '''Get the χ^2 best fit value of a0 and a1.'''\n",
    "\n",
    "    # Get the individual Σ values\n",
    "    Σy,Σx,Σx2,Σ1,Σxy = Σ(σ,y),Σ(σ,x),Σ(σ,x**2),Σ(σ,np.ones(x.size)),Σ(σ,x*y)\n",
    "\n",
    "    # the denominator\n",
    "    D = Σ1*Σx2 - Σx**2\n",
    "\n",
    "    # compute the best fit coefficients\n",
    "    a = np.array([Σy*Σx2 - Σx*Σxy,Σ1*Σxy - Σx*Σy])/D\n",
    "\n",
    "    # Compute the error in a\n",
    "    aErr = np.array([np.sqrt(Σx2/D),np.sqrt(Σ1/D)])\n",
    "\n",
    "    return a,aErr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"span alert alert-success\">\n",
    "<h2> Group Programming challenge </h2>\n",
    "Load the experimental data for the temperature T along a metal rod of length 10 cm\n",
    "    suspended between two constant temperature baths in file  <kbd>rod_temperature.dat</kbd>. Perform a linear fit to the data and display the resulting fit function and parameters in the legend.\n",
    "</div>\n",
    "\n",
    "<!--\n",
    "x,y,σ = np.loadtxt('../data/rod_temperature.dat',unpack=True)\n",
    "plt.errorbar(x,y,yerr=σ,linestyle='None', marker='o', capsize=0, elinewidth=1.5)\n",
    "a,aErr=get_a(x,y,σ)\n",
    "plt.plot(x,linear(x,a),label='fit',zorder=0)\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('L (cm)')\n",
    "plt.ylabel('Temperature (K)')\n",
    "plt.xlim(0,10)\n",
    "plt.legend(loc=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
